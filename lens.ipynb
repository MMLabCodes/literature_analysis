{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 70 # seed for NMF topic model\n",
    "num_topics = 12\n",
    "labels = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "vis_seed = 6 # seed for t-SNE visualization\n",
    "vis_angle = 135 # rotation angle for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition\n",
    "import sklearn.feature_extraction\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('paper')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# some python 3 trickery\n",
    "import sys\n",
    "if sys.version_info[0] >= 3:\n",
    "    unicode = str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fig(w=1, h=None):\n",
    "    if h is None: h = w\n",
    "    figsize = (6 * w, 3 * h)\n",
    "    sns.set(rc={'figure.figsize': figsize})\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.clf()\n",
    "    return fig\n",
    "\n",
    "def top_k(mapping, k=10):\n",
    "    return sorted(mapping.keys(), key=lambda x: mapping[x])[::-1][:k]\n",
    "\n",
    "pd.set_option('display.max_rows', 250)\n",
    "\n",
    "def plot_statistic(fun):\n",
    "    count = defaultdict(int)\n",
    "\n",
    "    for row in fun:\n",
    "        if row:\n",
    "            count[unicode(row)] += 1\n",
    "\n",
    "    top_keys = top_k(count, 50)\n",
    "\n",
    "    prepare_fig(1, 4)\n",
    "    plt.xlabel(\"No. publications\")\n",
    "    plt.barh(\n",
    "        range(len(top_keys)),\n",
    "        [count[a] for a in top_keys])\n",
    "    plt.yticks(\n",
    "        range(len(top_keys)), \n",
    "        [key[:50] for key in top_keys])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some query examples\n",
    "# \"protein AND (molecular dynamics) AND year_published:[2000 TO 2018]\"\n",
    "# \"author.display_name:(Jingjie AND Yeo) AND year_published:[2000 TO 2018]\"\n",
    "# \"(autonomous driving) OR (self-driving car) OR (robotic car)\"\n",
    "# \"(silk) OR (collagen) AND (biomaterial) AND (molecular dynamics)\"\n",
    "\n",
    "url = 'https://api.lens.org/scholarly/search'\n",
    "\n",
    "data = '''{\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"query\": \"(community participatory research) AND year_published:[2017 TO 2018]\",\n",
    "            \"default_operator\": \"and\"\n",
    "        }\n",
    "    },\n",
    "    \"size\": 500,\n",
    "    \"sort\": [\n",
    "            {\n",
    "            \"year_published\": \"desc\"\n",
    "            }\n",
    "     ]\n",
    "}'''\n",
    "\n",
    "# Use your LENS API key here\n",
    "headers = {'Authorization': 'Bearer YOUR_API_KEY', 'Content-Type': 'application/json'}\n",
    "\n",
    "response = requests.post(url, data=data, headers=headers)\n",
    "if response.status_code != requests.codes.ok:\n",
    "  print(response.status_code)\n",
    "else:\n",
    "  print(response.text)\n",
    "\n",
    "j = response.json()\n",
    "\n",
    "# Raw Data\n",
    "df_raw = pd.DataFrame.from_dict(j['data'])\n",
    "df_raw = df_raw.reset_index() # make sure indexes pair with number of rows\n",
    "\n",
    "# Filter journal articles only\n",
    "df = df_raw[df_raw['publication_type'].str.contains('journal article', na=False)] \n",
    "df = df.reset_index() # make sure indexes pair with number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start processing\n",
    "texts = []\n",
    "for index, row in df.iterrows():\n",
    "    text = (row['title'] or '') + ' ' \n",
    "    \n",
    "    # Skip over empty results\n",
    "    if not pd.isna(row['abstract']):\n",
    "        text = text + (row['abstract'] or '')\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text) # Replace punctation by spaces\n",
    "    texts.append([w for w in text.split(' ') if w]) # Split on spaces, remove empty items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications per year\n",
    "year_count = defaultdict(int)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Skip over empty results\n",
    "    if not pd.isna(row['year_published']):\n",
    "        year_count[int(row['year_published'])] += 1\n",
    "     \n",
    "years = range(2009, 2023)\n",
    "\n",
    "prepare_fig(1.8, 1.8)\n",
    "plt.ylabel(\"No. publications\",fontsize=18)\n",
    "plt.xlabel(\"Year\",fontsize=18)\n",
    "plt.bar(\n",
    "    years,\n",
    "    [year_count[y] for y in years])\n",
    "plt.xticks(years);\n",
    "plt.tick_params(labelsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig('years_ad.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications per type\n",
    "plot_statistic(df_raw['publication_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications per author\n",
    "author = []\n",
    "for a in df['authors']:\n",
    "    for b in a:\n",
    "        try:\n",
    "            author.append(b['last_name'] + ' ' + b['initials'])\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "plot_statistic(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications per institute\n",
    "def clean_affiliation(name):\n",
    "    name = unicode(name).title()\n",
    "    pairs = [\n",
    "        ['University', 'U'],\n",
    "        ['Universitat', 'U'],\n",
    "        ['Laboratories', 'Lab'],\n",
    "        ['Laboratory', 'Lab'],\n",
    "        ['National', 'Nat'],\n",
    "        ['Corporation', 'Corp'],\n",
    "        ['Technology', 'Tech'],\n",
    "        ['Institute', 'Inst'],\n",
    "        ['And', '&'],\n",
    "        ['Education', 'Ed'],\n",
    "        ['Engineering', 'Eng'],\n",
    "        ['Mechanical', 'Mech'],\n",
    "    ]\n",
    "    \n",
    "    for needle, replacement in pairs:\n",
    "        name = name.replace(needle, replacement)\n",
    "    return name\n",
    "\n",
    "inst = []\n",
    "for a in df['authors']:\n",
    "    for b in a:\n",
    "        \n",
    "        try:\n",
    "            b['affiliations']\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        for c in b['affiliations']:\n",
    "            affil = clean_affiliation(c['name'].split(',')[0])\n",
    "            inst.append(affil)\n",
    "            \n",
    "plot_statistic(inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications per publication source, conference/journal\n",
    "def clean_journal(name):\n",
    "    name = unicode(name).title()\n",
    "    pairs = [\n",
    "        ['Journal', 'J'],\n",
    "        ['Nature', 'Nat'],\n",
    "        ['Communications', 'Comm'],\n",
    "        ['Materials', 'Mat'],\n",
    "        ['Chemistry', 'Chem'],\n",
    "        ['Advanced', 'Adv'],\n",
    "        ['Technology', 'Tech'],\n",
    "        ['Institute', 'Inst'],\n",
    "        ['And', '&'],\n",
    "        ['Education', 'Ed'],\n",
    "        ['Engineering', 'Eng'],\n",
    "        ['Mechanical', 'Mech'],\n",
    "        ['International', 'Int'],\n",
    "        ['Biomaterials', 'Biomat'],\n",
    "        ['Science', 'Sci'],\n",
    "    ]\n",
    "    \n",
    "    for needle, replacement in pairs:\n",
    "        name = name.replace(needle, replacement)\n",
    "    return name\n",
    "\n",
    "pub_name = []\n",
    "for a in df['source']:\n",
    "    try:\n",
    "        a['title']\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "    pub_name.append(clean_journal(a['title']))\n",
    "            \n",
    "plot_statistic(pub_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords, bigrams, and stem rules\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(stop_words)\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "data_words = list(sent_to_words(texts))\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print common words\n",
    "one_count = defaultdict(int)\n",
    "\n",
    "for line in data_words_nostops:\n",
    "    for a in line:\n",
    "        one_count[a] += 1\n",
    "        \n",
    "print('Top words')\n",
    "display(pd.DataFrame(\n",
    "    [(w, one_count[w]) for w in top_k(one_count, 250)],\n",
    "    columns=['word', 'count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Print common bigrams\n",
    "two_count = defaultdict(int)\n",
    "\n",
    "for line in data_words_bigrams:\n",
    "    for a, b in zip(line, line[1:]):\n",
    "        two_count[a, b] += 1\n",
    "            \n",
    "print('Top bigrams')\n",
    "display(pd.DataFrame(\n",
    "    [(w, two_count[w]) for w in top_k(two_count, 250)],\n",
    "    columns=['bigram', 'count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "report_sentences_lemma = [' '.join(w) for w in data_lemmatized]\n",
    "\n",
    "large_string = ' '.join(report_sentences_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = gensim.corpora.Dictionary(data_lemmatized)\n",
    "dic.filter_extremes(0, 0.5) # Remove \n",
    "dic.filter_extremes(5, 1)   # \n",
    "corpus = [dic.doc2bow(line) for line in data_lemmatized]\n",
    "\n",
    "print('papers: {}'.format(len(corpus)))\n",
    "print('dictionary size: {}'.format(len(dic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency matrix\n",
    "n, m = len(corpus), len(dic)\n",
    "matrix = np.zeros((n, m))\n",
    "\n",
    "for i, row in enumerate(corpus):\n",
    "    for j, freq in row:\n",
    "        matrix[i,j] = freq\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TFIDF model\n",
    "tfidf_model = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "tfidf_matrix = tfidf_model.fit_transform(matrix).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = sklearn.decomposition.NMF(\n",
    "    n_components=num_topics,\n",
    "    random_state=seed,\n",
    "    tol=1e-9,\n",
    "    max_iter=500,\n",
    "    verbose=True)\n",
    "\n",
    "# Train model\n",
    "doc2topic = nmf_model.fit_transform(tfidf_matrix)\n",
    "topic2token = nmf_model.components_\n",
    "\n",
    "topic_norm = np.sum(topic2token, axis=1)\n",
    "topic2token /= topic_norm[:,np.newaxis]\n",
    "doc2topic *= topic_norm[np.newaxis,:]\n",
    "\n",
    "doc_norm = np.sum(doc2topic, axis=1)\n",
    "doc2topic /= doc_norm[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for label, vec in zip(labels, topic2token):\n",
    "    rows.append([label] + ['{} ({:.2})'.format(dic[i], vec[i]) for i in np.argsort(vec)[::-1][:10]])\n",
    "\n",
    "# Each row is a topic, columns are words ordered by weight \n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_fig(2, 3)\n",
    "for index in range(num_topics):\n",
    "    mapping = dict()\n",
    "    for i in np.argsort(topic2token[index])[::-1][:100]:\n",
    "        if topic2token[index,i] > 0:\n",
    "            mapping[dic[i]] = topic2token[index,i]\n",
    "    \n",
    "    def get_color(word, **kwargs):\n",
    "        weight = kwargs['font_size'] / 60.0 * 0.6 + 0.4\n",
    "        r, g, b = plt.get_cmap('Blues')(weight)[:3]\n",
    "        return 'rgb(%s, %s, %s)' % (int(r * 255), int(g * 255), int(b * 255))\n",
    "    \n",
    "    wc = WordCloud(\n",
    "        prefer_horizontal=True,\n",
    "        max_font_size=75,\n",
    "        #width=395,\n",
    "        #height=250,\n",
    "        scale=2,\n",
    "        background_color='white', \n",
    "        color_func=get_color, \n",
    "        relative_scaling=0.5)\n",
    "    wc.fit_words(mapping)\n",
    "    \n",
    "    print('Topic {} ({})'.format(index, labels[index]))\n",
    "    plt.subplot(4, 3, index + 1)\n",
    "    plt.imshow(wc.to_array(), interpolation='bilinear')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0.1, hspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold\n",
    "import sklearn.metrics.pairwise\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def draw_dot(p, t, zorder=0):\n",
    "    color = plt.get_cmap('jet')(float(t) / num_topics)\n",
    "    color = 0.8 * np.array(color)[:3]\n",
    "    \n",
    "    plt.scatter(\n",
    "        p[0], \n",
    "        p[1],\n",
    "        s=150,\n",
    "        c=[color],\n",
    "        marker='o',\n",
    "        linewidth=0.5,\n",
    "        zorder=zorder)\n",
    "    \n",
    "    plt.text(\n",
    "        p[0], \n",
    "        p[1],\n",
    "        labels[t],\n",
    "        fontsize=6,\n",
    "        color='1',\n",
    "        va='center',\n",
    "        ha='center',\n",
    "        fontweight='bold',\n",
    "        zorder=zorder + 1)\n",
    "\n",
    "# Lower dimensionality of original frequency matrix to improve cosine distances for visualization\n",
    "reduced_matrix = TruncatedSVD(\n",
    "    n_components=10, \n",
    "    random_state=seed\n",
    ").fit_transform(tfidf_matrix)\n",
    "\n",
    "# Learn model, tune perplexity value according to complexity of data\n",
    "model = sklearn.manifold.TSNE(\n",
    "    verbose=True,\n",
    "    metric='cosine',\n",
    "    random_state=vis_seed,\n",
    "    perplexity=5)\n",
    "pos = model.fit_transform(reduced_matrix)\n",
    "\n",
    "# Rotate visualization\n",
    "theta = np.deg2rad(vis_angle + 60)\n",
    "R = np.array([[np.cos(theta), np.sin(theta)], \n",
    "              [-np.sin(theta), np.cos(theta)]])\n",
    "pos = np.dot(pos, R)\n",
    "\n",
    "# Resize so xy-position is between 0.05 and 0.95\n",
    "pos -= (np.amin(pos, axis=0) + np.amax(pos, axis=0)) / 2\n",
    "pos /= np.amax(np.abs(pos))\n",
    "pos = (pos * 0.5) + 0.5\n",
    "pos = (pos * 0.9) + 0.05\n",
    "\n",
    "prepare_fig(2, 4)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "zorder = 0\n",
    "\n",
    "# Draw dots\n",
    "for i in np.random.permutation(len(doc2topic)):\n",
    "    topic_id = np.argmax(doc2topic[i])\n",
    "    draw_dot(pos[i], topic_id, zorder)\n",
    "    zorder += 2\n",
    "\n",
    "# Draw legend\n",
    "for i in range(num_topics):    \n",
    "    y = 0.985 - i * 0.02\n",
    "    label = ', '.join(dic[w] for w in np.argsort(topic2token[i])[::-1][:3])\n",
    "\n",
    "    draw_dot([0.015, y], i)\n",
    "    plt.text(0.03, y, label, ha='left', va='center', fontsize=8, zorder=zorder)\n",
    "    zorder += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an overall wordcloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=5000,\n",
    "    width=1500,\n",
    "    height=1000,\n",
    "    stopwords=stop_words,\n",
    "    contour_width=3,\n",
    "    contour_color='steelblue'\n",
    ")\n",
    "\n",
    "# display our wordcloud across all records\n",
    "plt.figure(figsize=(16,16))\n",
    "word_cloud.generate(large_string)\n",
    "word_cloud.to_file(\"all_words.png\")\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyblio",
   "language": "python",
   "name": "pyblio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
