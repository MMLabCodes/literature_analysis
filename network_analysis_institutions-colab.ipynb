{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b099faa",
   "metadata": {},
   "source": [
    "# Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 70 # seed for NMF topic model\n",
    "num_topics = 12\n",
    "labels = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "vis_seed = 6 # seed for t-SNE visualization\n",
    "vis_angle = 135 # rotation angle for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc9df7",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d028fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition\n",
    "import sklearn.feature_extraction\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('paper')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# some python 3 trickery\n",
    "import sys\n",
    "if sys.version_info[0] >= 3:\n",
    "    unicode = str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190017f",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfb5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fig(w=1, h=None):\n",
    "    if h is None: h = w\n",
    "    figsize = (6 * w, 3 * h)\n",
    "    sns.set(rc={'figure.figsize': figsize})\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.clf()\n",
    "    return fig\n",
    "\n",
    "def top_k(mapping, k=10):\n",
    "    return sorted(mapping.keys(), key=lambda x: mapping[x])[::-1][:k]\n",
    "\n",
    "pd.set_option('display.max_rows', 250)\n",
    "\n",
    "def plot_statistic(fun):\n",
    "    count = defaultdict(int)\n",
    "\n",
    "    for row in fun:\n",
    "        if row:\n",
    "            count[unicode(row)] += 1\n",
    "\n",
    "    top_keys = top_k(count, 50)\n",
    "\n",
    "    prepare_fig(1, 4)\n",
    "    plt.xlabel(\"No. publications\")\n",
    "    plt.barh(\n",
    "        range(len(top_keys)),\n",
    "        [count[a] for a in top_keys])\n",
    "    plt.yticks(\n",
    "        range(len(top_keys)), \n",
    "        [key[:50] for key in top_keys])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d60e0",
   "metadata": {},
   "source": [
    "# Perform Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91834765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some query examples\n",
    "# \"protein AND (molecular dynamics) AND year_published:[2000 TO 2018]\"\n",
    "# \"author.display_name:(Jingjie AND Yeo) AND year_published:[2000 TO 2018]\"\n",
    "# \"(autonomous driving) OR (self-driving car) OR (robotic car)\"\n",
    "# \"(silk) OR (collagen) AND (biomaterial) AND (molecular dynamics)\"\n",
    "# \"(community participatory research) AND year_published:[2017 TO 2018]\"\n",
    "\n",
    "url = 'https://api.lens.org/scholarly/search'\n",
    "\n",
    "request_body = '''{\n",
    "\t\"query\": {\n",
    "\t\t\"query_string\": {\n",
    "\t\t\t\"query\": \"(silk) OR (collagen) AND (biomaterial) AND (molecular dynamics)\",\n",
    "            \"default_operator\": \"and\"\n",
    "\t\t}\n",
    "\t},\n",
    "    \"languages\": \"en\",\n",
    "    \"scroll\": \"1m\",\n",
    "    \"size\": 500,\n",
    "    \"sort\": [\n",
    "            {\n",
    "            \"year_published\": \"desc\"\n",
    "            }\n",
    "     ]\n",
    "}'''\n",
    "\n",
    "headers = {'Authorization': 'Bearer YOUR_API_KEY', 'Content-Type': 'application/json'}\n",
    "\n",
    "df_raw = pd.DataFrame()\n",
    "# Recursive function to scroll through paginated results\n",
    "def scroll(scroll_id, url, request_body, headers):\n",
    "    global df_raw\n",
    "    # Change the request_body to prepare for next scroll api call\n",
    "    # Make sure to append the include fields to make faster response\n",
    "    if scroll_id is not None:\n",
    "        request_body = '''{\"scroll_id\": \"%s\"}''' % (scroll_id)\n",
    "\n",
    "    # make api request\n",
    "    response = requests.post(url, data=request_body, headers=headers) \n",
    "\n",
    "    # If rate-limited, wait for n seconds and proceed the same scroll id\n",
    "    # Since scroll time is 1 minutes, it will give sufficient time to wait and proceed\n",
    "    if response.status_code == requests.codes.too_many_requests:\n",
    "        time.sleep(8)\n",
    "        scroll(scroll_id, url, request_body, headers)\n",
    "  \n",
    "    # If the response is not ok here, better to stop here and debug it\n",
    "    elif response.status_code != requests.codes.ok:\n",
    "        print(response.json())\n",
    "  \n",
    "    # If the response is ok, do something with the response, take the new scroll id and iterate\n",
    "    else:\n",
    "        json = response.json()\n",
    "\n",
    "        # End recursion once end of search is reached\n",
    "        if not json['data']:\n",
    "            return\n",
    "\n",
    "        scroll_id = json['scroll_id'] # Extract the new scroll id from response\n",
    "        \n",
    "        # Store the data into a dataframe\n",
    "        print(json['data'])\n",
    "        df_raw = df_raw.append(pd.DataFrame.from_dict(json['data'])) \n",
    "        \n",
    "        # Keep scrolling\n",
    "        scroll(scroll_id, url, request_body, headers)\n",
    "\n",
    "# start recursive scrolling\n",
    "scroll(None, url, request_body, headers)\n",
    "\n",
    "# Raw Data\n",
    "df_raw = df_raw.reset_index() # make sure indexes pair with number of rows\n",
    "\n",
    "# Filter journal articles only\n",
    "df = df_raw[df_raw['publication_type'].str.contains('journal article', na=False)] \n",
    "df = df.reset_index() # make sure indexes pair with number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7551569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process data to obtain author information        \n",
    "from itertools import combinations\n",
    "\n",
    "# Publications per institute\n",
    "def clean_affiliation(name):\n",
    "    name = unicode(name).title()\n",
    "    pairs = [\n",
    "        ['University', 'U'],\n",
    "        ['Universitat', 'U'],\n",
    "        ['Laboratories', 'Lab'],\n",
    "        ['Laboratory', 'Lab'],\n",
    "        ['National', 'Nat'],\n",
    "        ['Corporation', 'Corp'],\n",
    "        ['Technology', 'Tech'],\n",
    "        ['Institute', 'Inst'],\n",
    "        ['And', '&'],\n",
    "        ['Education', 'Ed'],\n",
    "        ['Engineering', 'Eng'],\n",
    "        ['Mechanical', 'Mech'],\n",
    "        ['Department', 'Dept'],\n",
    "    ]\n",
    "    \n",
    "    for needle, replacement in pairs:\n",
    "        name = name.replace(needle, replacement)\n",
    "    return name\n",
    "\n",
    "# Get each manuscript's institutions as a Pandas Series\n",
    "insts = pd.DataFrame()\n",
    "for a in df['authors'].dropna():\n",
    "    try:\n",
    "        inst_list = []\n",
    "        for b in a:\n",
    "        \n",
    "            try:\n",
    "                b['affiliations']\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "            for c in b['affiliations']:\n",
    "                affil = clean_affiliation(c['name'].split(',')[0])\n",
    "                inst_list.append(affil)\n",
    "                \n",
    "        inst = []\n",
    "        inst.append(list(set(inst_list)))\n",
    "        insts = insts.append(pd.Series(inst), ignore_index=True)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# We will extract institution connections\n",
    "# First, convert to Pandas Series\n",
    "insts = insts.squeeze()\n",
    "\n",
    "# Create a list of these authors\n",
    "inst_flat = [\n",
    "    inst\n",
    "    for insts in list(insts.dropna())\n",
    "    for inst in insts\n",
    "]\n",
    "\n",
    "# Permute all combinations of institution pairs in each manuscript\n",
    "inst_connections = list(\n",
    "    map(lambda x: list(combinations(x[::-1], 2)), insts)\n",
    ")\n",
    "\n",
    "# Flatten into a list\n",
    "flat_connections = [item for sublist in inst_connections for item in sublist]\n",
    "\n",
    "# Create a dataframe with the connections\n",
    "df_connect = pd.DataFrame(flat_connections, columns=[\"From\", \"To\"])\n",
    "df_graph = df_connect.groupby([\"From\", \"To\"]).size().reset_index()\n",
    "df_graph.columns = [\"From\", \"To\", \"Count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadf373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graphs to connect each manuscript's institutions\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df_graph, source=\"From\", target=\"To\", edge_attr=\"Count\"\n",
    ")\n",
    "\n",
    "# Limit to top institutions, please edit accordingly\n",
    "top_inst = pd.DataFrame.from_records(\n",
    "    Counter(inst_flat).most_common(30), columns=[\"Name\", \"Count\"]\n",
    ")\n",
    "\n",
    "top_nodes = (n for n in list(G.nodes()) if n in list(top_inst[\"Name\"]))\n",
    "\n",
    "G_top = G.subgraph(top_nodes)\n",
    "\n",
    "for n in G_top.nodes():\n",
    "    G_top.nodes[n][\"publications\"] = int(\n",
    "        top_inst[top_inst[\"Name\"] == n][\"Count\"]\n",
    "    )\n",
    "\n",
    "# Get degree of centrality of nodes\n",
    "d = dict(G_top.degree)\n",
    "\n",
    "# Get edges and their weights\n",
    "edges = G_top.edges()\n",
    "weights = [G_top[u][v]['Count'] for u,v in edges]\n",
    "\n",
    "# Normalize the edge weights for better visualization\n",
    "xmin = min(weights) \n",
    "xmax = max(weights)\n",
    "for i, x in enumerate(weights):\n",
    "    weights[i] = (x) / (xmax-xmin)\n",
    "    \n",
    "# Adjust figsize accordingly to get optimal figure labels\n",
    "fig, ax = plt.subplots(figsize=(20,15))\n",
    "ax.axis('equal')\n",
    "\n",
    "# Adjust node_size and fonts accordingly to get optimal nodes and labels\n",
    "nx.draw_circular(G_top, \n",
    "                 node_size=[v * 100 for v in d.values()], \n",
    "                 with_labels=True, font_weight='bold', font_size=12, \n",
    "                 width=weights)\n",
    "\n",
    "plt.savefig(\"institutions_graph.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD: Nicer visualizations using NXVIZ\n",
    "# Currently incomplete so YMMV\n",
    "!pip install nxviz\n",
    "import nxviz as nv\n",
    "from nxviz import annotate\n",
    "from nxviz.utils import edge_table, node_table \n",
    "from nxviz.plots import aspect_equal, despine\n",
    "\n",
    "node_table(G_top)\n",
    "edge_table(G_top)\n",
    "\n",
    "ax = nv.circos(G_top)\n",
    "annotate.circos_labels(G_top)\n",
    "despine()\n",
    "aspect_equal()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
